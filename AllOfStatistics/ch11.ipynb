{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baysian method: the model parameter $\\theta$ is also a r.v. and has its pdf.\n",
    "1. prior distribution PDF=$f(\\theta)$\n",
    "2. Choose a statistical model $f(x|\\theta)$ (f(x) given $\\theta$), as opposed to $f(x;\\theta)$ (parameterized by $\\theta$).\n",
    "3. Calculate the posterior distribution $f(\\theta | x)$.\n",
    "\n",
    "Optimize according to the posterior distribution $f(\\theta | x)$, which is equivalent to the frequentist point of view of $f(\\theta; x)$. \n",
    "\n",
    "We denote n observations $(X_1,...,X_n)$ as $X^n$, the likelihood of n observations $f(x|\\theta)$ is just\n",
    "$$\n",
    "f(x_1,...,x_n|\\theta)=\\prod f(x_i|\\theta)=\\mathcal{L}_n(\\theta)\n",
    "$$\n",
    "\n",
    "The posterior PDF is:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "f(\\theta|x^n)&=&\\frac{f(x^n|\\theta) f(\\theta)}{f(x^n)} \\\\\n",
    "&=&\\frac{f(x^n|\\theta) f(\\theta)}{\\int f(x^n,\\theta)d\\theta} \\\\\n",
    "&=&\\frac{f(x^n|\\theta) f(\\theta)}{\\int f(x^n|\\theta)f(\\theta) d\\theta} \\\\\n",
    "&=&\\frac{f(x^n|\\theta) f(\\theta)}{c_n} \\\\\n",
    "&\\propto& f(x^n|\\theta) f(\\theta) =  \\mathcal{L}_n(\\theta) f(\\theta) \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "Conclusion: posterior is propotional to likelihood times prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage of posterior\n",
    "1. posterior mean: sum the posterior over prior.\n",
    "$$\n",
    "\\overline{\\theta}_n =\\int \\theta f(\\theta|x^n) d\\theta \n",
    "= \\int \\frac{\\theta \\mathcal{L}_n(\\theta) f(\\theta)}{\\mathcal{L}_n(\\theta) f(\\theta)} d\\theta\n",
    "$$\n",
    "2. posterior interval: find the $\\alpha$ interval C=[a,b] where:\n",
    "$$\n",
    "\\int_{-\\infty}^{a} f(\\theta|x^n)d\\theta = \\int_{b}^{\\infty} f(\\theta|x^n)d\\theta = \\alpha/2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.1 Example\n",
    "$X_i\\sim \\text{Bernoulli}(p)$ and prior $f(p)=1$\n",
    "1. the posterior  \n",
    "$$\n",
    "f(p|x^n) \\propto f(p)\\mathcal{L}_n(p)=p^s(1-p)^{n-s}\n",
    "$$\n",
    "\n",
    "2. posterior mean.  \n",
    "Beta distribution PDF is:\n",
    "$$\n",
    "f(p;\\alpha,\\beta)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)+\\Gamma(\\beta)} p^{\\alpha-1}(1-p)^{\\beta-1}\n",
    "$$\n",
    "Rewrite the posterior as:  \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "f(p|x^n) &\\propto& p^s(1-p)^{n-s} \\\\\n",
    "&=& p^{(s+1)-1}(1-p)^{(n-s+1) - 1} \\\\\n",
    "&\\propto& \\frac{\\Gamma(n+2)}{\\Gamma(s+1)\\Gamma(n-s+1)} p^s(1-p)^{n-s}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "It can be concluded that:  \n",
    "$$\n",
    "p|x^n \\sim \\text{Beta}(s+1, n-s+1)\n",
    "$$\n",
    "The posterior mean here is just the mean of beta $$\\frac{\\alpha}{\\alpha+\\beta}=\\frac{s+1}{n+2}$$.  \n",
    "Question: How is the constant $c_n=\\int f(x^n|\\theta)f(\\theta) d\\theta$ figured out?    \n",
    "\n",
    "* Posterior mean can be written in terms of MLE $\\hat{p}$ and prior mean $\\tilde{p}$:  \n",
    "$$\n",
    "\\overline{p}=\\lambda_n \\hat{p} + (1-\\lambda_n) \\tilde{p}, \\text{ where } \\lambda_n=\\frac{n}{n+2}\n",
    "$$\n",
    "\n",
    "* Use $p \\sim \\text{Beta}(\\alpha,\\beta)$ as prior, the posterior is hence:  \n",
    "$$\n",
    "f(p|x^n) \\propto f(p)\\mathcal{L}_n(p) \n",
    "= \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)+\\Gamma(\\beta)} p^{s+\\alpha-1}(1-p)^{n-s+\\beta-1}\n",
    "$$\n",
    "which means $p|x^n \\sim \\text{Beta}(\\alpha+s,\\beta+n-s)$, the normalizing constant is $c_n=\\frac{\\Gamma(\\alpha+\\beta+n)}{\\Gamma(\\alpha+s)\\Gamma(\\beta+n-s)}$  \n",
    "Hence the posterior mean is $\\overline{p}=\\frac{\\alpha+s}{\\alpha+\\beta+n}$. The prior mean is just $\\frac{\\alpha}{\\alpha+\\beta}$;    \n",
    "The uniform prior is just a special case of Beta prior.  \n",
    "\n",
    "When the prior and posterior are in the same distribution family, we say that the prior is $\\textbf{conjugate}$ with respect to it's posterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.2 Example\n",
    "$X_i \\sim N(\\theta,\\delta^2)$ and $\\delta^2$ is known; take the prior $\\theta \\sim N(a,b^2)$. Show that \n",
    "$$\n",
    "\\theta|X^n \\sim N(\\overline{\\theta},\\tau^2)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\theta=w \\overline{X} + (1-w)a \\\\\n",
    "w=\\frac{\\frac{1}{se^2}}{\\frac{1}{se^2}+\\frac{1}{b^2}}, \\frac{1}{\\tau^2}=\\frac{1}{se^2}+\\frac{1}{b^2}, se=\\delta/\\sqrt{n}\n",
    "$$\n",
    "As $n \\longrightarrow \\infty$, we have $se \\longrightarrow 0, w \\longrightarrow 1, \\tau/se \\longrightarrow 1$. So if n is large, the posterior is approximately $N(\\hat{p}, se^2)$. Here the MLE $\\hat{p}=\\overline{X}$.  \n",
    "The 95% confidence interval of the posterior can be given as  \n",
    "$$\n",
    "C_{0.95}=\\overline{\\theta}\\pm 1.96 \\tau\n",
    "$$\n",
    "Given the fact that $\\overline{\\theta} \\approx \\hat{\\theta}$ and $\\tau \\approx se$, we have \n",
    "$$\n",
    "C_{0.95}=\\hat{\\theta}\\pm 1.96 se\n",
    "$$\n",
    "Which is just the same as the frequentists' confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function parameters\n",
    "Make inference about a function $\\tau=g(\\theta)$.  \n",
    "Given a r.v. X and its pdf $f_X$, the density of Y=g(X)=$F_Y'(x)$  \n",
    "The CDF of Y is given by: \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "F_Y(y)&=&\\mathbb{P}(Y\\le y) \\\\\n",
    "&=& \\mathbb{P}(g(X) \\le y) \\\\\n",
    "&=& \\int_{g(X) \\le y} f(x)dx\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "The same rule applies to posterior PDF.\n",
    "\n",
    "11.3 Example \n",
    "$X_i\\sim \\text{Bernoulli}(p)$ and prior $f(p)=1$; the posterior $p|x^n \\sim \\text{Beta}(s+1, n-s+1)$;  \n",
    "Let $\\psi=\\log \\frac {p}{1-p}$, it's posterior?  \n",
    "From 11.1 we know that:\n",
    "$$\n",
    "f(p)=\\frac{\\Gamma(n+2)}{\\Gamma(s+1)\\Gamma(n-s+1)} p^s(1-p)^{n-s}\n",
    "$$\n",
    "The CDF of $\\psi=\\log \\frac {p}{1-p}$ is:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "H(\\psi|x^n) &=&\\mathbb{P}(\\Psi \\le \\psi) \\\\\n",
    "&=&\\mathbb{P}(\\log \\frac {p}{1-p} \\le \\psi) \\\\\n",
    "&=&\\mathbb{P}(p \\le \\frac{e^{\\psi}}{e^{\\psi}+1}) \\\\\n",
    "&=& \\int_{-\\infty}^{\\frac{e^{\\psi}}{e^{\\psi}+1}} f(p|x^n)dp\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "the PDF of $\\psi$ is:  \n",
    "$$\n",
    "h(\\psi|x^n) = H'(\\psi|x^n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above examples are just some well known close form posterior distributions that can be written in formulas. In general there is no close form representation for a posterior, and it is not easy to do it computationally (it often involves complicated integration; and one example of dealing such complexity is EM algorithm).   \n",
    "Another way is to do it with simulation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.5 Theorem  \n",
    "Let $\\hat{\\theta}$ be the MLE and $\\hat{se}=\\sqrt{nI(\\hat{\\theta})}$. Under appropriate regularity conditions, the posterior is approximately Normal with mean $\\hat{\\theta}_n$ and stardard error $\\hat{se}$. Hence $\\overline{\\theta}_n=\\hat{\\theta}_n$.  \n",
    "Also if $C_n=\\hat{\\theta} \\pm z_{\\alpha/2} \\hat{se}$ is the frequentist $1-\\alpha$ confidence interval, it is also approximately  $1-\\alpha$ Beysian confidence interval.  \n",
    "There is also a Beysian delta method. Let $\\tau=g(\\theta)$, then  \n",
    "$$\n",
    "\\tau|x^n \\sim N(\\hat{\\tau}, \\overline{se}^2)\n",
    "$$\n",
    "where $\\hat{\\tau}=g(\\hat{\\theta})$ and $\\overline{se}=\\hat{se}*g'(\\hat{\\theta})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.6 Flat Priors, Improper Priors and \"Noninformative\" Priors\n",
    "\n",
    "1. Non-informative prior: an example is the flat prior as in 11.1, $f(p)=1$\n",
    "2. Improper prior: suppose we adopt $f(p)=c$, it ingegrates to infinity; it is called 'improper prior'.   \n",
    "If $X_i \\sim N(\\theta,\\delta^2)$ and the prior $f(p)=c$, the resulting posterior is $\\theta|X^n \\sim N(\\overline{\\theta},\\tau^2)$ which agrees exactly with their frequentist counterparts. This indicates that even though the prior is improper, it can still be used as a valid prior in bayes statistics.  \n",
    "3. Flat priors are not invariant. \n",
    "If $X_i \\sim \\text{Bernoulli}(p) $ and prior $f(p)=1$. Now let $\\psi=\\log \\frac {p}{1-p}$. The prior PDF after conversion is just  \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "F_{\\Psi}(\\psi)&=&\\mathbb{P}(\\Psi \\le \\psi) \\\\\n",
    "&=& \\mathbb{P}(\\log \\frac {p}{1-p} \\le \\psi) \\\\\n",
    "&=& \\mathbb{P}(p \\le \\frac{e^{\\psi}}{1+e^{\\psi}}) \\\\\n",
    "&=& \\int_{-\\infty}^{\\frac{e^{\\psi}}{1+e^{\\psi}}} f(p) dp \\\\\n",
    "&=& \\int_{-\\infty}^{\\frac{e^{\\psi}}{1+e^{\\psi}}} dp \\\\\n",
    "f_{\\Psi}(\\psi) &=& \\frac{e^{\\psi}}{(1+e^{\\psi})^2}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "This means a flat prior after transforming to other r.v. is no longer flat. It is not transformation invariant.  \n",
    "4. Jeffrey's Prior: $f(\\theta) \\propto I(\\theta)^{1/2}$. It is transformation invariant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.7 Example\n",
    "For Bernoulli model, the fisher information is $I(p)=\\frac{1}{p(1-p)}$.   \n",
    "Jeffrey's prior is hence $f(p) \\propto \\frac{1}{\\sqrt{p(1-p)}}$.   \n",
    "It is a Beta(1/2,1/2) density, close to a uniform pdf.  \n",
    "Prove that Jeffrey's prios is transformation invariant.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.9 Strengths and weaknesses of Beysian Inference\n",
    "In parametric inference, with large samples, both frequentist and beysian methods give approximately the same inference. \n",
    "\n",
    "11.8 Let $\\theta$ be fixed known real, $X_1$,$X_2$ be independent r.v. such that $\\mathbb{P}(X_i=1)=\\mathbb{P}(X_i=-1)=1/2$. Now define $Y_i=\\theta+X_i$ and suppose only observe $Y_1,Y_2$. Let \n",
    "$$ \n",
    "C = \n",
    "\\begin{cases}\n",
    "  {Y_1-1}                 & \\text{if } Y_1=Y_2 \\\\\n",
    "  {\\frac{Y_1+Y_2}{2}}     & \\text{if } Y_1 \\ne Y_2\n",
    "\\end{cases}\n",
    "$$\n",
    "No matter what $\\theta$ is, $\\mathbb{P}(\\theta \\in C)=3/4$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
