{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Convergence of R.V."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Introduction\n",
    "\n",
    "Suppose that $X_1,X_2,...$ is a sequence of R.V. which are independent and are from N(0,1), we can not simply conclude that $\\lim_{n\\rightarrow \\infty}X_n=X \\sim N(0,1)$, because $\\mathbb{P}(X_n=X)=0$ for all n.   \n",
    "Another example, suppose $X_n \\sim N(0,1/n)$. Intuitively we would like to say that $\\lim_{n \\rightarrow}X_n=0$. But it is not true, because $\\mathbb{P}(X_n=0)=0$ for all n.   \n",
    "\n",
    "In order to discuss convergence in a rigorous way, there are two main ideas.  \n",
    "1. The law of layge numbers: the sample average $\\bar X_n$ converges in probability to the expectation $\\mu=\\mathbb{E}(X_i)$.  \n",
    "2. The central limit theorem: $\\sqrt n (\\bar X_n - \\mu)$ converges in distribution to a Normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Types of Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Definition\n",
    "$X_i$, $X$ is a sequence of R.V.; there CDF are denoted as $F_i$ and $F$ respectively.\n",
    "1. $X_n$ converges to $X$ in provbability: $X_n \\overset{p}{\\longrightarrow} X$, if $\\forall \\epsilon \\gt 0$,\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|X_n - X|\\gt \\epsilon) = 0\n",
    "$$\n",
    "\n",
    "\n",
    "2. $X_n$ converges to $X$ in distribution: $X_n \\rightsquigarrow X$, if $\\forall t$ at which $F$ is continuous: \n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} F_n(t)=F(t)\n",
    "$$\n",
    "\n",
    "\n",
    "3. $X_n$ converges to $X$ in quadratic mean: $X_n \\overset{qm}{\\longrightarrow} X\\$, if\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} \\mathbb{E}(X_n-X)^2=0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3 Example\n",
    "Let $X_n \\sim N(0,1/n)$. Intuitively $X_n$ converges to 0, but it will be interpreted in a different way compared to the way it is in calculus. (Use Point mass distribution to prove it.)\n",
    "1. Convergence in distribution: $X_n \\rightsquigarrow 0$ \n",
    "2. Convergence in probability: $X_n \\overset{p}{\\longrightarrow} 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4 Theorem\n",
    "1. $X_n \\overset{qm}{\\longrightarrow} X\\ \\Rightarrow X_n \\overset{p}{\\longrightarrow} X$ \n",
    "2. $X_n \\overset{p}{\\longrightarrow} X\\ \\Rightarrow X_n \\rightsquigarrow X$ \n",
    "3. $X_n \\rightsquigarrow X$ and $\\mathbb{P}(X=c)=1 \\Rightarrow X_n \\overset{p}{\\longrightarrow} X\\$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explaination. $X_n \\overset{p}{\\longrightarrow} b \\ne \\mathbb{E}(X_n)\\rightarrow b$  \n",
    "Use the following example: \n",
    "$$\n",
    "\\mathbb{P}(X_n=n^2)=\\frac{1}{n}\n",
    "$$\n",
    "For the above R.V.:  \n",
    "$X_n \\overset{p}{\\longrightarrow} 0$ but $\\mathbb{E}(X_n)\\rightarrow \\infty$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.5 Theorem\n",
    "Let $g$ be a continuous function.\n",
    "1. $X_n \\overset{p}{\\longrightarrow} X , Y_n \\overset{p}{\\longrightarrow} Y \\Rightarrow X_n+Y_n \\overset{p}{\\longrightarrow} X+Y$\n",
    "2. $X_n \\overset{qm}{\\longrightarrow} X , Y_n \\overset{qm}{\\longrightarrow} Y \\Rightarrow X_n+Y_n \\overset{qm}{\\longrightarrow} X+Y$\n",
    "3. $X_n \\rightsquigarrow X , Y_n \\rightsquigarrow c \\Rightarrow X_n+Y_n \\rightsquigarrow X+c$\n",
    "4. $X_n \\overset{p}{\\longrightarrow} X , Y_n \\overset{p}{\\longrightarrow} Y \\Rightarrow X_n Y_n \\overset{p}{\\longrightarrow} X Y$\n",
    "5. $X_n \\rightsquigarrow X , Y_n \\rightsquigarrow c \\Rightarrow X_n Y_n \\rightsquigarrow cX$\n",
    "6. $X_n \\overset{p}{\\longrightarrow} X \\Rightarrow g(X_n) \\overset{p}{\\longrightarrow} g(X)$\n",
    "7. $X_n \\rightsquigarrow X \\Rightarrow g(X_n) \\rightsquigarrow g(X)$\n",
    "\n",
    "Parts 3,5 are known as Slutzky's theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 The Law of Large Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6 Theorem (The Weak Law of Large Numbers (WLLN))\n",
    "Let $X_i$ be an IID sample, let $\\mu = \\mathbb{E}(X_i)$ and $\\delta^2 = \\mathbb{V}(X_i)$, we already know that $\\mathbb{E}(\\overline{X}_n)=\\mu$ and $\\mathbb{V}(\\overline{X}_n)=\\frac{\\delta^2}{n}$. The weak law of large numbers says:\n",
    "$$\n",
    "\\overline{X}_n \\overset{P}{\\longrightarrow} \\mu\n",
    "$$\n",
    "\n",
    "Proof can be given by Chebyshev's inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.7 Theorem (The Central Limit Theorem (CLT))\n",
    "Let $X_i$ be an IID sample, let $\\mu = \\mathbb{E}(X_i)$ and $\\delta^2 = \\mathbb{V}(X_i)$, then\n",
    "$$\n",
    "Z_n \\equiv \\frac{\\overline{X}_n - \\mu}{\\sqrt{\\mathbb{V}(\\overline{X}_n)}} = \\frac{\\sqrt{n}(\\overline{X}_n-\\mu)}{\\delta}\\rightsquigarrow Z\n",
    "$$\n",
    "In other words, $\\overline{X}_n$ converges in distribution to a normal.  \n",
    "In language of machine leanring, it says the more samples the less variance.  \n",
    "\n",
    "A question to CLT is that the varialce $\\delta^2$ is rarely known. Normally the variance is estimated by sample variance $S_n=\\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\overline{X}_n)^2 $   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.10 Theorem\n",
    "Assume the same conditions as the CLT. Then\n",
    "$$\n",
    "\\frac{\\sqrt{n}(\\overline{X}_n)-\\mu}{S_n} \\rightsquigarrow N(0,1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.11 Theorem (The Berry-Esseen Inequality)\n",
    "Suppose that $\\mathbb{E}|X_i|^3 \\lt \\infty$, then\n",
    "$$\n",
    "\\sup_z |\\mathbb{P}(Z_n \\lt z)-\\Phi(z)| \\le \\frac{33}{4} \\frac{\\mathbb{E}|X_i-\\mu|^3}{\\sqrt{n} \\delta^3}\n",
    "$$\n",
    "The B-E inequality gives the accuracy of the CLT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 The Delta Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.13 Theorem (The Delta Method)\n",
    "Suppose that\n",
    "$$\n",
    "\\frac{\\sqrt{n}(\\overline{X}_n-\\mu)}{\\delta}\\rightsquigarrow Z\n",
    "$$\n",
    "and $g$ is a differentiable function s.t. $g'(\\mu)\\ne 0$. Then\n",
    "$$\n",
    "\\frac{\\sqrt{n}(g(\\overline{X}_n)-g(\\mu))}{|g'(\\mu)|\\delta}\\rightsquigarrow Z\n",
    "$$\n",
    "\n",
    "Here we need to make use of Taylor's expansion to prove the theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.14 Example.\n",
    "Let $X_i$ be IID with mean $\\mu$ and variance $\\delta^2$. Let $W_n=e^{\\overline{X}_n}$. Thus $W_n=g(\\overline{X}_n)$ where $g(s)=e^s$, the delta method implies that $W_n \\approx N(e^{\\mu}, \\frac{e^{2\\mu}\\delta^2}{n})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
