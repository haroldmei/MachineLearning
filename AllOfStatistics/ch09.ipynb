{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Parametric Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametric models:\n",
    "$$\n",
    "\\mathfrak{F}=\\{ f(x; \\theta): \\theta \\in \\Theta \\}\n",
    "$$\n",
    "\n",
    "Why is nonparametric methods are preferable in statistical inference? Because we rarely know that the data is generated from a specific distribution.   \n",
    "\n",
    "Two reasons make studing parametric models useful. \n",
    "1. Some well known scenarios, such as counts of traffic accidents, follow specific models.\n",
    "2. Parametric models provide background for understanding certain nonparametric methods.\n",
    "\n",
    "Topics:  \n",
    "1. Parameters of interst\n",
    "2. Two methods: the methods of moments and method of maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Parameter of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In $X \\sim N(\\mu,\\delta^2)$, the parameter is $\\theta = (\\mu,\\delta)$. If we are only interested in $\\mu$, it can be seen as $\\mu = T(\\theta)$ and is called $\\textbf{parameter of interest}$, and $\\delta$ is called $\\textbf{nuisance parameter}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.1 Example  \n",
    "\n",
    "Let $X_i \\sim N(\\mu, \\delta^2)$. the parameter is $\\theta=(\\mu,\\delta)$ and the parameter space is $\\Theta={(\\mu,\\delta): \\mu \\in \\mathbb{R}, \\delta > 0}$. Suppose $X_i$ is the outcome of a blood test and suppose we are interested in $\\tau$, the fraction of the population whose test score is larger than 1. Then\n",
    "$$\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\tau=\\mathbb{P}(X>1) &=& 1-\\mathbb{P}(X<1) \\\\\n",
    "&=& 1 - \\mathbb{P}(\\frac{X-\\mu}{delta}<\\frac{1-\\mu}{\\delta}) \\\\\n",
    "&=& 1 - \\mathbb{P}(Z<\\frac{1-\\mu}{\\delta}) \\\\\n",
    "&=& 1 - \\Phi(\\frac{1-\\mu}{\\delta})\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "The parameter of interest is $\\tau=T(\\mu, \\delta)=1-\\Phi(\\frac{1-\\mu}{\\delta})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.2 Example  \n",
    "\n",
    "Recall that $X \\sim Gamma(\\alpha,\\beta)$ if\n",
    "$$\n",
    "f(x;\\alpha,\\beta)=\\frac{1}{\\beta^\\alpha \\Gamma (\\alpha)} x^{\\alpha-1} e^{x/\\beta},x>0\n",
    "$$\n",
    "where $\\alpha,\\beta>0$ and the Gamma function is\n",
    "$$\n",
    "\\Gamma (\\alpha)=\\int_0^\\infty y^{\\alpha-1}e^{-y} dy\n",
    "$$\n",
    "The parameter $\\theta=(\\alpha,\\beta)$. It is sometimes used to model lifetimes of people, animals and electronic equipment. The mean lifetime is $T(\\alpha,\\beta)=\\mathbb{E}_\\theta (X_1)=\\alpha \\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 The Method of Moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not optimal but easy to compute. Suppose that the parameter $\\theta=(\\theta_1,...,\\theta_k)$ has $k$ components. For $1\\le j \\le k$, define the jth moment\n",
    "$$\n",
    "\\alpha_j \\equiv \\alpha_j(\\theta)=\\mathbb{E}(X^j)=\\int x^j dF(x)\n",
    "$$\n",
    "and the jth sample moment\n",
    "$$\n",
    "\\hat{\\alpha}_j(\\theta)=\\frac{1}{n}\\sum_{i=1}^n X_i^j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.3 Definition\n",
    "The method of momnets estimator $\\hat{\\theta}_n$ is defined to be the value of $\\theta$ such that\n",
    "$$\n",
    "\\alpha_1(\\hat{\\theta}_n)=\\hat{\\alpha}_1 \\\\\n",
    "\\alpha_2(\\hat{\\theta}_n)=\\hat{\\alpha}_2 \\\\\n",
    "... \\\\\n",
    "\\alpha_k(\\hat{\\theta}_n)=\\hat{\\alpha}_k \\\\\n",
    "$$\n",
    "there are k equations with k unknowns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.5 Example  \n",
    "\n",
    "Let $X_i \\sim N(\\mu, \\delta^2)$.  Then $\\alpha_1=\\mathbb{E}_\\theta (X_1)=\\mu, \\alpha_2=\\mathbb{E}_\\theta (X_1^2)=\\delta^2+\\mu^2$. Solve the following equations can get us $\\mu,\\delta$:\n",
    "$$\n",
    "\\begin{cases}\n",
    "  \\mu            &= \\frac{1}{n}\\sum_{i=1}^n X_i \\\\\n",
    "  \\delta^2+\\mu^2 &= \\frac{1}{n}\\sum_{i=1}^n X_i^2\n",
    "\\end{cases}\n",
    "$$\n",
    "This is a system of 2 equations with 2 unknowns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.6 Theorem\n",
    "Let $\\hat{\\theta}_n$ denote the method of moments estimator. Under appropriate conditions on the model, the following statements hold:  \n",
    "1. The estimate $\\hat{\\theta}_n$ exists with probability thending to 1.\n",
    "2. The estimate is consistent: $\\hat{\\theta}_n \\overset{P}{\\longrightarrow} \\theta$.\n",
    "3. The estimate is asympototically Normal:\n",
    "$$\n",
    "\\sqrt{n}(\\hat{\\theta}_n-\\theta) \\rightsquigarrow N(0, \\Sigma)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\Sigma = g \\mathbb{E}_\\theta (YY^T) g^T, Y=(X,X^2,...,X^k)^T, g=(g_1,g_2,...,g_k) and g_j=\\frac{\\partial a_j^{-1}(\\theta)}{\\partial \\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.7 Definition (likelihood function )\n",
    "The likelihood function is defined by \n",
    "$$\n",
    "\\mathcal{L}(\\theta)=\\prod_{i=1}^n f(X_i;\\theta)\n",
    "$$\n",
    "\n",
    "The log-likelihood function is defined by\n",
    "$$\n",
    "\\ell(\\theta)=\\log \\mathcal{L}(\\theta) = \\sum_{i=1}^n f(X_i;\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.8 Definition (maximum likelihood estimator)\n",
    "\n",
    "The maximum likelihood estimator MLE is denoted by $\\hat{\\theta}_n$ s.t.\n",
    "$$\n",
    "\\hat{\\theta}_n = \\arg \\max \\mathcal{L}(\\theta)\n",
    "$$\n",
    "The maximum of $\\ell(\\theta)$ occurs at the same place as $\\mathcal{L}(\\theta)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.12 Example (A Hard Example)\n",
    "\n",
    "Let $X_i \\sim \\text{Uniform}(0, \\theta)$. \n",
    "$$ \n",
    "f(x;\\theta) = \n",
    "\\begin{cases}\n",
    "  \\frac{1}{\\theta} & 0 \\le x \\le \\theta \\\\\n",
    "  0                & otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Suppose $X_{(n)}=\\max{X_1,...,X_n}$, the likelihood can be written as:\n",
    "$$\n",
    "\\mathcal{L}(\\theta)=\n",
    "\\begin{cases}\n",
    "  (\\frac{1}{\\theta})^n & \\theta \\ge X_{(n)} \\\\\n",
    "  0                & \\theta \\lt X_{(n)}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Properties of MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main properties (under certain conditions) are:\n",
    "1. The MLE is consistent;\n",
    "2. The MLE is equivariant: if $\\hat{\\theta}_n$ is the MLE of \\theta, then $g(\\hat{\\theta}_n)$ is the MLE of $g(\\theta)$;\n",
    "3. The MLE is asymptotically Normal, and the estimated standard error $\\hat{se}$ can often be computed analytically;\n",
    "4. The MLE is asymptotically optimal or efficient. Roughly this means that among all well-behaved estimators, the MLE has the smallest variance, at least for large samples.\n",
    "5. The MLE is approximately the Bayes estimator.\n",
    "\n",
    "In sufficiently complicated problems these will no longer hold and the MLE will no longer be a good estimator.  \n",
    "The properties only hold if the model satifies certain regularity conditions, these are essentially smoothness conditions on $f(x;\\theta)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 Consistency of MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kullback-Leibler distance\n",
    "Suppose $f$ and $g$ are PDFs, define the Kullback-Leibler distance between $f$ and $g$ to be\n",
    "\n",
    "$$\n",
    "D(f,g)=\\int f(x) \\log \\left(\\frac{f(x)}{g(x)} \\right) dx\n",
    "$$\n",
    "\n",
    "A model $\\mathfrak{F}$ is identifiable if $\\theta \\ne \\psi$ implies that $D(\\theta, \\psi) \\gt 0$, this means that different values of the parameter correspond to different distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\theta_*$ denote the true value of $\\theta$, consider it as a fixed constant, maximizing $ell_n(\\theta)$ is equivalent to maximizing\n",
    "$$\n",
    "M_n(\\theta)=\\frac{1}{n} \\sum_i \\log \\frac{f(X_i;\\theta)}{f(X_i;\\theta_*)}\n",
    "$$\n",
    "\n",
    "By the LLN, $M_n(\\theta)$ converges to\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\mathbb{E}_{\\theta_*} \\left (\\log \\frac{f(X_i;\\theta)}{f(X_i;\\theta_*)} \\right )\n",
    "&=& \\int \\log \\frac{f(X_i;\\theta)}{f(X_i;\\theta_*)} f(X_i;\\theta_*) dx \\\\\n",
    "&=& -\\int \\log \\frac{f(X_i;\\theta_*)}{f(X_i;\\theta)} f(X_i;\\theta_*) dx \\\\\n",
    "&=& -D(\\theta_*, \\theta)\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "Hence $M_n(\\theta)$ is maximized at $\\theta_*$. To prove this formally, we need more than $M_n(\\theta) \\overset{P}{\\longrightarrow} -D(\\theta_*, \\theta)$. We need this convergence to be uniform over $\\theta$. We also have to make sure that the function $D(\\theta_*,\\theta)$ is well behaved.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.14 Theorem\n",
    "Let $\\theta_*$ denote the true value of $\\theta$. Define\n",
    "$$\n",
    "M_n(\\theta)=\\frac{1}{n} \\sum_i \\log \\frac{f(X_i;\\theta)}{f(X_i;\\theta_*)}\n",
    "$$\n",
    "and $M(\\theta)=-D(\\theta_*,\\theta)$. Suppose that\n",
    "$$\n",
    "\\sup_{\\theta \\in \\Theta} \\left | M_n(\\theta)-M(\\theta) \\right | \\overset{P}{\\longrightarrow} 0 \\tag{9.7}\n",
    "$$\n",
    "and that, for every $\\epsilon \\gt 0$,\n",
    "$$\n",
    "\\sup_{\\theta:\\left | \\theta - \\theta_* \\right | \\ge \\epsilon } M(\\theta) \\lt M(\\theta_*) \\tag{9.8}\n",
    "$$\n",
    "let $\\hat{\\theta}_n$ denote the MLE.\n",
    "Then $\\hat{\\theta}_n \\overset{P}{\\longrightarrow} \\theta_*$\n",
    "\n",
    "This theorem says that uniform consistency of $M_n(\\theta) \\overset{P}{\\longrightarrow} -D(\\theta_*, \\theta)$ over $\\theta$ implies that the MLE is a consistent estimator of the true value $\\theta_*$. The inequality (9.8) says that KL divergence $D(\\theta_*,\\theta)$ has only one minimum (well behaved). To prove it, just show that the MLE is clapped by $M_n(\\theta_*)$ and $M(\\theta_*)$ and apply (9.7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6 Equivariance of the MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.14 Theorem\n",
    "Let $\\tau=g(\\theta)$ be a function of $\\theta$. Let $\\hat{\\theta}_n$ be the MLE of $\\theta$. Then $\\hat{\\tau}_n=g(\\hat{\\theta}_n)$ is the MLE of $\\tau$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.15 Example\n",
    "Let $X_i \\sim N(\\theta,1)$, the MLE for $\\theta$ is $\\overline{X}_n$; let $\\tau=e^\\theta$, then the MLE for $\\tau$ is $e^{\\overline X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7 Asymptotic Normality\n",
    "\n",
    "It turns out that the distribution of $\\hat{\\theta}_n$ is approximately Normal and we can compute its aproximate variance analytically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.16 Definition (score function and fisher information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.17 Theorem. $I_n(\\theta)=n I(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.18 Theorem (Asymptotic Normality of the MLE)\n",
    "Let $ se = \\sqrt{\\mathbb{V}(\\hat{\\theta}_n)}$. Under appropriate regularity conditions, the following hold:\n",
    "1. $se \\approx \\sqrt \\frac{1}{I_n(\\theta)}$ and\n",
    "$$\n",
    "\\frac{\\hat{\\theta}_n-\\theta}{se} \\rightsquigarrow N(0,1)\n",
    "$$\n",
    "2. Let $\\hat{se} = \\sqrt \\frac{1}{I_n(\\theta)}$. Then,\n",
    "$$\n",
    "\\frac{\\hat{\\theta}_n-\\theta}{\\hat{se}} \\rightsquigarrow N(0,1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.19 Theorem (asymptotic confidence interval)\n",
    "From 9.18, $\\hat{\\theta}_n \\sim N(\\theta,\\hat{se}^2)$ we can write the $1-\\alpha$ confidence interval\n",
    "$$\n",
    "C_n=\\left(\\hat{\\theta}_n-z_{\\alpha/2}\\hat{se},\\hat{\\theta}_n+z_{\\alpha/2}\\hat{se} \\right)\n",
    "$$\n",
    "Then, $\\mathbb{P}(\\theta \\in C_n) \\overset{P}{\\longrightarrow} \\infty $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.20 Example\n",
    "Let $X_i \\sim \\text{Bernoulli}(p)$\n",
    "1. MLE\n",
    "2. PDF\n",
    "3. Score function\n",
    "4. Fisher information\n",
    "5. $\\hat{se}$\n",
    "6. 95 percent confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.21 Example\n",
    "Let $X_i \\sim N(\\theta, \\delta^2)$ where $\\delta^2$ is known.\n",
    "1. MLE\n",
    "2. PDF\n",
    "3. Score function\n",
    "4. Fisher information\n",
    "5. $\\hat{se}$\n",
    "6. 95 percent confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.22 Example\n",
    "Let $X_i \\sim Poisson(\\lambda)$.\n",
    "1. MLE\n",
    "2. PDF\n",
    "3. Score function\n",
    "4. Fisher information\n",
    "5. $\\hat{se}$\n",
    "6. 95 percent confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.8 Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $X_i \\sim N(\\theta, \\delta^2)$. The MLE is $\\hat{\\theta}_n=\\overline{X}_n$. It satisfies asymptotic normality  \n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_n \\sim N(\\theta,\\hat{se}^2)\n",
    "$$\n",
    "\n",
    "Another reasonable estimator of $\\theta$ is the sample median $\\tilde{\\theta}_n$. It can be proved that the median satisfies\n",
    "$$\n",
    "\\tilde{\\theta}_n \\sim N(\\theta, \\frac{\\pi \\delta^2}{2n})\n",
    "$$\n",
    "It means the median has a larger variance compared to MLE.\n",
    "\n",
    "Question: proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition (asymptotic relative efficiency)\n",
    "Consider two estimators $T_n$ and $U_n$ and suppose that\n",
    "$$\n",
    "\\sqrt{n}(T_n-\\theta)\\rightsquigarrow N(0,t^2) \\\\\n",
    "\\sqrt{n}(U_n-\\theta)\\rightsquigarrow N(0,u^2)\n",
    "$$\n",
    "the asymptitoc relative efficiency of $U$ and $T$ is defined by\n",
    "$$\n",
    "\\text{ARE}(T,U)=\\frac{t^2}{u^2}\n",
    "$$\n",
    "In the Normal example, $\\text{ARE}(\\tilde{\\theta}_n,\\hat{\\theta}_n)=2/\\pi = 0.63$. It means that if you use median, you are effectively using only a fraction of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.23 Theorem\n",
    "If $$ is the MLE and $$ is any other estimator, then\n",
    "$$\n",
    "\\text{ARE}(\\tilde{\\theta}_n,\\hat{\\theta}_n)=\\frac{t^2}{u^2} \\le 1\n",
    "$$\n",
    "Thus, the MLE has the smallest (asymptotic) variance and we say that the MLE is efficient or asymptotic optimal.  \n",
    "Question: proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.9 The Delta Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\tau=g(\\theta)$ where g is a smooth function. The maximum likelihood estimator of $\\tau$ is $\\hat{\\tau}=g(\\hat{\\theta})$. What is the distribution of $\\hat{\\tau}$?\n",
    "\n",
    "#### 9.24 Theorem (The Delta Method)\n",
    "If $\\tau=g(\\theta)$ where g is differentiable and $g'(\\theta)\\ne 0$ then\n",
    "$$\n",
    "\\frac{\\hat{\\tau}_n-\\tau }{\\hat{se}(\\hat{\\tau})} \\rightsquigarrow N(0,1)\n",
    "$$\n",
    "where $\\hat{\\tau}_n=g(\\hat{\\theta}_n)$ and\n",
    "$$\n",
    "\\hat{se}(\\hat{\\tau})=\\left| g'(\\theta) \\right| \\hat{se}(\\hat{\\theta}_n)\n",
    "$$\n",
    "Hence, if \n",
    "$$\n",
    "C_n=\\left(\\hat{\\tau}_n-z_{\\alpha/2}\\hat{se}(\\hat{\\tau}_n),\\hat{\\tau}_n+z_{\\alpha/2}\\hat{se}(\\hat{\\tau}_n) \\right)\n",
    "$$\n",
    "Then, $\\mathbb{P}(\\tau \\in C_n) \\overset{P}{\\longrightarrow} \\infty $.\n",
    "\n",
    "Question: use Taylor's expansion to prove it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.25 Example\n",
    "Let $X_i \\sim \\text{Bernoulli}(p)$ and let $\\psi=g(p)=\\log(p/(1-p))$.\n",
    "1. $\\hat{se}$\n",
    "2. 95 percent confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.26 Example\n",
    "Let $X_i \\sim N(\\theta, \\delta^2)$ where $\\mu$ is known and $\\delta^2$ is unknown and we want to estimate $\\psi=\\log \\delta$.\n",
    "1. $\\hat{se}$\n",
    "2. 95 percent confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.10 Multiparameter Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.11 The Parametric Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sufficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.32 Definition\n",
    "1. Definition from All Of Statistics  \n",
    "Write $x^n \\leftrightarrow y^n$ if $f(x^n;\\theta)=cf(y^n;\\theta)$ for some constant c that might depend on $x^n$ and $y^n$ but not $\\theta$. A statistic $T(x^n)$ is sufficient if $T(x^n) \\leftrightarrow T(y^n)$ implies that $x^n \\leftrightarrow y^n$. Note that if $x^n \\leftrightarrow y^n$, then the likelihood function based on $x^n$ has the same shape as the likelihood function based on $y^n$. Roughly speaking if a statistic is sufficient if we can calculate the likelihood function knowing only $T(X^n)$.   \n",
    "\n",
    "\n",
    "2. Definition from STAT705 lecture notes:  \n",
    "Suppose that $X_1,...,X_n \\sim p(x;\\theta)$. T is sufficient for $\\theta$ if the conditional distribution of $X_1,...,X_n|T$ does not depend on $\\theta$. In other words, \n",
    "$$\n",
    "p(x_1,...,x_n|t;\\theta)=p(x_1,...,x_n|t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.33 Example\n",
    "Let $X_i \\sim \\text{Bernoulli}(p)$. Then $\\mathcal{L}(p)=p^S (1-p)^{n-S}$ where $S=\\sum_i X_i$, so S is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.34 Example\n",
    "Let $X_i \\sim N(\\theta, \\delta^2)$ and let $T=(\\overline{X}, S)$. Then\n",
    "$$\n",
    "f(X^n;\\mu,\\delta)=\\left( \\frac{1}{\\delta \\sqrt{2\\pi}} \\right)^n \n",
    "\\exp \\left\\{-\\frac{nS^2}{2\\delta^2} \\right\\} \n",
    "\\exp \\left\\{ -\\frac{n(\\overline{X}-\\mu)^2}{2 \\delta^2} \\right\\}\n",
    "$$\n",
    "where S is the sample variance. The last expression depends on the data only through $T$ and therefore, $T=(\\overline{X},S)$ is a sufficient statistic.  \n",
    "Which of the following are sufficient?\n",
    "1. $T(X^n)=(X_1,...,X_n)$\n",
    "2. $T(X^n)=(\\overline{X},S)$\n",
    "3. $T(X^n)=\\overline{X}$\n",
    "4. $T(X^n)=(\\overline{X},S,X_3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.35 Definition (minimal sufficient MSS)\n",
    "A statistic $T$ is minimal sufficient if\n",
    "1. it is sufficient\n",
    "2. it is a function of every other sufficient statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.36 Theorem\n",
    "$T$ is minimal sufficient if the following is true:\n",
    "$$\n",
    "T(x^n)=T(y^n) \\iff x^n \\leftrightarrow y^n\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sufficient Partitions\n",
    "A statistic induces a partition on the set of outcomes. We can think of sufficient in terms of these partitions.  \n",
    "\n",
    "#### Example\n",
    "Let $X_1,X_2,X_3 \\sim \\text{Bernoulli}(\\theta)$. Let $T=\\sum_iX_i$. (Statistic $U$ in the following table illustrates a sufficient but not minimal statistic. )    \n",
    "\n",
    "\n",
    "| $X_1$ | $X_2$ | $X_3$ |  t  |  $p(x|t)$  |  u  |  $p(x|u)$  |\n",
    "|-------|-------|-------|-----|------------|-----|------------|\n",
    "|   0   |   0   |   0   |  0  |     1      |  0  |     1      |\n",
    "|-------|-------|-------|-----|------------|-----|------------|\n",
    "|   0   |   0   |   1   |  1  |    1/3     |  1  |    1/3     |\n",
    "|   0   |   1   |   0   |  1  |    1/3     |  1  |    1/3     |\n",
    "|   1   |   0   |   0   |  1  |    1/3     |  1  |    1/3     |\n",
    "|-------|-------|-------|-----|------------|-----|------------|\n",
    "|   0   |   1   |   1   |  2  |    1/3     |  2  |    1/1     |\n",
    "|   1   |   0   |   1   |  2  |    1/3     |  2  |    1/1     |\n",
    "|   1   |   1   |   0   |  2  |    1/3     |  4  |    1       |\n",
    "|-------|-------|-------|-----|------------|-----|------------|\n",
    "|   1   |   1   |   1   |  3  |    1       |  3  |    1       |\n",
    "|-------|-------|-------|-----|------------|-----|------------|\n",
    "\n",
    "\n",
    "1. A partition $B_1,...,B_k$ is sufficient if $f(x|X \\in B)$ does not depend on $\\theta$. The above $p(x|t)$ are all constant and does not depend on the model parameter $\\theta$.\n",
    "2. A statistic $T$ induces a partition; $T$ is sufficient iff the partition is sufficient. \n",
    "3. Two statistic can generate the same partition, example: $\\sum_i X_i$ and $3 \\sum_i X_i$.\n",
    "4. If we split any element $B_i$ of a sufficient partition into smaller pieces, we get another sufficient partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another example, same $X_i$ but a different statistic $T=X_1$. The partition is then\n",
    "\n",
    "\n",
    "| $X_1$ | $X_2$ | $X_3$ |  t  |  $p(x|t)$  |\n",
    "|-------|-------|-------|-----|------------|\n",
    "|   0   |   0   |   0   |  0  | $(1-\\theta)^2$ |\n",
    "|   0   |   0   |   1   |  0  | $\\theta(1-\\theta)$ |\n",
    "|   0   |   1   |   0   |  0  | $\\theta(1-\\theta)$ |\n",
    "|   0   |   1   |   1   |  0  | $\\theta^2$ |\n",
    "|-------|-------|-------|-----|------------|\n",
    "|   1   |   0   |   0   |  1  | $(1-\\theta)^2$ |\n",
    "|   1   |   0   |   1   |  1  | $\\theta(1-\\theta)$ |\n",
    "|   1   |   1   |   0   |  1  | $\\theta(1-\\theta)$ |\n",
    "|   1   |   1   |   1   |  1  | $\\theta^2$ |\n",
    "|-------|-------|-------|-----|------------|\n",
    "\n",
    "The outcome conditioned on statistic partition isn't dependent on model parameter so it is not sufficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.37 Example\n",
    "Let $X_1,X_2 \\sim \\text{Bernoulli}(\\theta)$. Let $V=X_1,T=\\sum_iX_i$ and $U=(T,X_1)$. Here is the set of outcomes and the statistics  \n",
    "\n",
    "| $X_1$ | $X_2$ |  V  |  T  |  U  |\n",
    "|-----|-----|-----|-----|-----|\n",
    "|0|0|0|0|(0,0)|\n",
    "|0|1|0|1|(1,0)|\n",
    "|1|0|1|1|(1,1)|\n",
    "|1|1|1|2|(2,1)|\n",
    "\n",
    "The partitions induced by these statistics are:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "V &\\longrightarrow& \\{(0,0),(0,1)\\},\\{(1,0),(1,1)\\}, p(x|v)=\\{\\theta, 1-\\theta \\},\\{\\theta, 1-\\theta \\}  \\\\\n",
    "T &\\longrightarrow& \\{(0,0)\\},\\{(0,1),(1,0)\\},\\{(1,1)\\}, p(x|t)=\\{1 \\},\\{1/2, 1/2 \\},\\{1 \\}  \\\\\n",
    "U &\\longrightarrow& \\{(0,0)\\},\\{(0,1)\\},\\{(1,0)\\},\\{(1,1)\\}, p(x|u)=\\{1 \\},\\{1\\}, \\{1 \\},\\{1 \\} \n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "The statistic V isn't sufficient since the outcome conditioned on it isn't dependent on its model parameter.  \n",
    "U is sufficient but not minimal. Consider $x_n=(0,1),y^n=(1,0)$, we have $f(x^n)=f^(y^n)=\\theta(1-\\theta)$, but $T(x^n)!=T(y_n)$.   \n",
    "T is minimal sufficient. Because given any $x^n,y^n$, as long as $f(x^n)\\leftrightarrow f^(y^n)$ (meaning $f(x^n)=cf^(y^n)$), we would have $T(x^n)=T(y^n)$. Shown below  \n",
    "\n",
    "| $X^n$ | $f(x_n)$ |\n",
    "|-------|----------|\n",
    "| (0,0) |$(1-\\theta)^2$|\n",
    "| (0,1) |$\\theta(1-\\theta)$|\n",
    "| (1,0) |$\\theta(1-\\theta)$|\n",
    "| (1,1) |$\\theta^2$|\n",
    "\n",
    "In above table, $x_n=(0,0)$ does not have any $y^n$ that satisfies $T(x^n)=T(y^n)$ other than itself, this is the same as $x_n=(1,1)$. For $x_n=(1,0)$ and $y_n=(0,1)$, we have $T(x_n)=T(y_n)=\\{(0,1),(1,0)\\}$ and $f(x_n)=f(y_n)$. According to 9.36, it is minimal sufficient.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to find a MSS (* From CMU STAT705 lecture notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem\n",
    "Define $R(x_n,y_n;\\theta)=\\frac{p(y^n;\\theta)}{p(x_n;\\theta)}$. Suppose that $T$ has the following property:  \n",
    "\n",
    "$$\n",
    "R(x_n,y_n;\\theta) = R(x_n,y_n) \\iff T(x_n)=T(y^n)\n",
    "$$\n",
    "Then $T$ is minimal sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "Let $Y_1,...,Y_n \\sim Poisson(\\theta)$.\n",
    "$$\n",
    "p(y^n;\\theta)=\\frac{e^{-n\\theta}\\theta \\sum_i y_i}{\\prod_i y_i},\n",
    "\\frac{p(y^n;\\theta)}{p(x^n;\\theta)}=\\frac{\\theta \\left(\\sum_i y_i-\\sum_i x_i \\right)}{\\frac{\\prod_i{y_i !}}{\\prod_i{x_i !}}}\n",
    "$$\n",
    "which is independent of $\\theta$ iff $\\sum_i y_i=\\sum_i x_i$, which implies that $T(Y^n)=\\sum_i Y_i$ is a minimal sufficient statistic for $\\theta$.  \n",
    "\n",
    "\n",
    "If $T$ is sufficient, then $T$ contains all the information you need from the data to compute the likelihood function. It does not contain all the information in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.38 Example\n",
    "Check the correctless of the following statements.  \n",
    "1. For the Normal model, $T=(\\overline{X},S)$ is minimal sufficient.  \n",
    "2. For the Bernoulli model, $T=\\sum_i X_i$ is minimal sufficient.  \n",
    "3. For the Poisson model, $T=\\sum_i X_i$ is minimal sufficient, $T=(\\sum_i X_i, X_1)$ is sufficient but not minimal.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.39 Example\n",
    "Two coin flips. Let $X=(X_1,X_2) \\sim \\text{Bernoulli}(p)$. Then $T=X_1+X_2$ is sufficient.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.40 Theorem (Factorization)\n",
    "$T$ is sufficient of $X_n$ iff there are functions $g(t;\\theta)$ and $h(x)$ such that $f(x^n;\\theta)=g(t;\\theta)h(x^n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "Let $Y_1,...,Y_n \\sim Poisson(\\theta)$. $T=\\sum_i Y_i$ is sufficient\n",
    "$$\n",
    "p(y^n;\\theta)=\\frac{e^{-n\\theta}\\theta \\sum_i y_i}{\\prod_i y_i}\n",
    "$$\n",
    "Here $g(t;\\theta)=e^{-n\\theta}\\theta t$; $h(y^n)=\\prod_i y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "Let $X_i \\sim N(\\theta, \\delta^2)$;\n",
    "1. If $\\delta$ is known, show that $\\overline{X}$ is sufficient for $\\mu$.\n",
    "2. If $(\\mu, \\delta^2)$ is unknown, show that $T=(\\overline{X},S^2)$ is sufficient.\n",
    "3. If $(\\mu, \\delta^2)$ is unknown, show that $T=(\\sum_i X_i, \\sum_i X_i^2)$ is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.42 Theorem (Rao-Blackwell)\n",
    "Let $\\hat{\\theta}$ be an estimator and let $T$ be a sufficient statistic. Define a new estimator by \n",
    "\n",
    "$$\n",
    "\\tilde{\\theta}=\\mathbb{E}(\\hat{\\theta}|T)\n",
    "$$\n",
    "\n",
    "Then, for every $\\theta$, $R(\\theta,\\tilde{\\theta})\\le R(\\theta, \\hat{\\theta})$. Here $R(\\theta, \\hat{\\theta})$ is just the MSE (Mean Square Error) of the estimator $\\hat{\\theta}$ (More generally it is a risk function, and MSE is just one kind of risk).  \n",
    "\n",
    "The Rao-Blackwell theory says that an estimator should only depend on the sufficient statistic, otherwise it can be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.43 Example\n",
    "Flip coins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Families\n",
    "\n",
    "Contents are covered in Machine Learning courses.\n",
    "1. Exponential family\n",
    "2. Newton's method\n",
    "3. EM algorithm\n",
    "4. Mixture of Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
