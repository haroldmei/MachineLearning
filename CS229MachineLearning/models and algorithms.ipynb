{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $m$ target variables and there inputs $(y^{(i)}, x^{(i)})$ are linearly related with an error term $\\epsilon^{(i)}$:\n",
    "$$y^{(i)} = \\theta^T x^{(i)} + \\epsilon^{(i)}$$\n",
    "The error term $\\epsilon^{(i)}$ is an IID drawn from normal distribution:\n",
    "$$\\epsilon^{(i)} \\sim N(0,\\delta^2)$$\n",
    "$$y^{(i)}|x^{(i)} \\sim N(\\theta^T x^{(i)}, \\delta^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally weighted linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard Linear Regression is trying to minimize $\\sum_i(y^{(i)}-\\theta^T x^{(i)})^2$;  \n",
    "The LWR is trying to minimize: $\\sum_i w^{(i)} (y^{(i)}-\\theta^T x^{(i)})^2$. One choice of the weights is:\n",
    "$$ w^{(i)} = \\exp\\left(-\\frac{(x^{(i)}-x)^2}{2\\tau^2}\\right)$$\n",
    "Which means the closer a point is to the target point $x$, the larger the weight. A larger weight corresponds to a point means more contribute of that point to the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable $y^{(i)}$ is a bernoulli:\n",
    "$$y^{(i)} \\sim \\text{Bernoulli}(h_\\theta(x^{(i)}))$$\n",
    "where $$ h_\\theta(x^{(i)}) = \\frac{1}{1+\\exp(-\\theta^T x^{(i)})}$$\n",
    "The likelihood of the parameters is:\n",
    "$$L\n",
    "(\\theta) = p(\\overrightarrow{y}|X;\\theta)\n",
    "=\\prod_{i=1}^m{h_\\theta(x^{(i)})^{y^{(i)}}(1-h_\\theta(x^{(i)}))^{1-y^{(i)}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Models; The Exponential Family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminative learning: Learn a conditional probability $p(y|x)$. Both linear regiression and logistic regression are such algorithms.  \n",
    "Generative learning: learn $p(x|y)$ and $p(y)$ and then come up with the joint probability $p(x,y)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y \\sim \\text{Bernoulli}(\\phi) \\\\\n",
    "x|y=0 \\sim N(\\mu_0, \\Sigma) \\\\\n",
    "x|y=1 \\sim N(\\mu_1, \\Sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naives Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event models for text classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
