{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative models\n",
    "Model the \"conditional distribution\" $p(y^{(i)}|x^{(i)})$ by writing down a model.   \n",
    "\n",
    "Linear regression:  \n",
    "$$y^{(i)}|x^{(i)} \\sim N(\\theta^T x^{(i)}, \\delta^2) $$\n",
    "\n",
    "Logistic regression:  \n",
    "$$\n",
    "y^{(i)}|x^{(i)} \\sim \\text{Bernoulli}\\left(\\frac{1}{1+\\exp(-\\theta^T x^{(i)})}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "Given $m$ target variables and there inputs $(y^{(i)}, x^{(i)})$ are linearly related with an error term $\\epsilon^{(i)}$:\n",
    "$$y^{(i)} = \\theta^T x^{(i)} + \\epsilon^{(i)}$$\n",
    "The error term $\\epsilon^{(i)}$ is an IID drawn from normal distribution:\n",
    "$$\\epsilon^{(i)} \\sim N(0,\\delta^2)$$\n",
    "$$y^{(i)}|x^{(i)} \\sim N(\\theta^T x^{(i)}, \\delta^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "The target variable $y^{(i)}$ is a bernoulli:\n",
    "$$y^{(i)}|x^{(i)} \\sim \\text{Bernoulli}(h_\\theta(x^{(i)}))$$\n",
    "where $$ h_\\theta(x^{(i)}) = \\frac{1}{1+\\exp(-\\theta^T x^{(i)})}$$\n",
    "The likelihood of the parameters is:\n",
    "$$L\n",
    "(\\theta) = p(\\overrightarrow{y}|X;\\theta)\n",
    "=\\prod_{i=1}^m{h_\\theta(x^{(i)})^{y^{(i)}}(1-h_\\theta(x^{(i)}))^{1-y^{(i)}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally weighted linear regression\n",
    "The standard Linear Regression is trying to minimize $\\sum_i(y^{(i)}-\\theta^T x^{(i)})^2$;  \n",
    "The LWR is trying to minimize: $\\sum_i w^{(i)} (y^{(i)}-\\theta^T x^{(i)})^2$. One choice of the weights is:\n",
    "$$ w^{(i)} = \\exp\\left(-\\frac{(x^{(i)}-x)^2}{2\\tau^2}\\right)$$\n",
    "Which means the closer a point is to the target point $x$, the larger the weight. A larger weight corresponds to a point means more contribute of that point to the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron learning\n",
    "Instead of using logistic function, set $g(z)$ as:  \n",
    "$$ \n",
    "g(z) = \n",
    "\\begin{cases}\n",
    "  1                     & \\text{for } z \\ge 0 \\\\\n",
    "  0     & \\text{for } z \\lt 0\n",
    "\\end{cases}\n",
    "$$\n",
    "And \n",
    "$$h_{\\theta}(x)=g(\\theta^Tx)$$\n",
    "Use the same update rule:\n",
    "$$\n",
    "\\theta_j := \\theta_j + \\alpha(y^{(i)}-h_{\\theta}(x^{(i)}))x_j^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Models, the Exponential Family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Learning Algorithms\n",
    "Discriminative learning: Learn a conditional probability $p(y|x)$.   \n",
    "\n",
    "Generative learning: learn $p(x|y)$ and $p(y)$ and then come up with the joint probability $p(x,y)$. Write the model w.r.t. $y$ and $x|y$ first then learn with maximum log likelihood $\\ell(\\Theta)$.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Discriminant Analysis\n",
    "Model form:\n",
    "$$\n",
    "y \\sim \\text{Bernoulli}(\\phi) \\\\\n",
    "x|y=0 \\sim N(\\mu_0, \\Sigma) \\\\\n",
    "x|y=1 \\sim N(\\mu_1, \\Sigma)\n",
    "$$\n",
    "\n",
    "Write out the probabilities accordingly:\n",
    "$$\n",
    "p(y)=\\phi^y(1-\\phi)^{1-y} \\\\\n",
    "p(x|y=0)=\\frac{1}{\\sqrt{(2\\pi)^n|\\Sigma|}}\\exp\\left(-\\frac{1}{2}(x-\\mu_0)^T\\Sigma^{-1}(x-\\mu_0) \\right) \\\\\n",
    "p(x|y=1)=\\frac{1}{\\sqrt{(2\\pi)^n|\\Sigma|}}\\exp\\left(-\\frac{1}{2}(x-\\mu_1)^T\\Sigma^{-1}(x-\\mu_1) \\right) \\\\\n",
    "$$\n",
    "\n",
    "The log likelihood is:\n",
    "$$\n",
    "\\ell(\\phi,\\mu_0,\\mu_1,\\Sigma) = \\log \\prod_{i=1}^n p(x^{(i)}, y^{(i)}; \\phi, \\mu_0, \\mu_1, \\Sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naives Bayes in text classification\n",
    "\n",
    "An $n$ words vocabulary, each text is denoted as an $n$ dimensional vector $x^{(i)} \\in \\mathbb{R}^n$. The following NB model has a total number of $2n$+1 parameters.  \n",
    "Model form:\n",
    "$$\n",
    "y^{(i)} \\sim \\text{Bernoulli}(\\phi) \\\\\n",
    "x_j^{(i)}|y^{(i)}=0 \\sim \\text{Bernoulli}(\\phi_{j|y=0}) \\\\\n",
    "x_j^{(i)}|y^{(i)}=1 \\sim \\text{Bernoulli}(\\phi_{j|y=1}) \\\\\n",
    "$$\n",
    "The conditional distribution parameter is given by:\n",
    "$$\n",
    "\\phi_{j|y=0}=p(x_j=1|y=0)\n",
    "$$\n",
    "\n",
    "Assume words in the text are conditionally independent to each other:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "p(x|y) &=& p(x_1, ..., x_n|y) \\\\ \n",
    "&=&p(x_1|y)p(x_2|y)...p(x_n|y) \\\\\n",
    "&=& \\prod_j^n p(x_j|y)\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "Then the log-likelihood function can be written as: \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\ell(\\phi, \\phi_{j|y=0}, \\phi_{j|y=1})\n",
    "&=& \\log \\prod p(x^{(i)}, y^{(i)}) \\\\\n",
    "&=& \\log \\prod p(x^{(i)}|y^{(i)}) p(y^{(i)})\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "The part $x|y \\sim \\text{bernoulli}(\\phi_j)$ can be replaced by multinomial distributions, to model a Naive Bayes for continuous data by discretizing it first.\n",
    "\n",
    "\n",
    "#### Laplace smoothing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event models for text classification\n",
    "\n",
    "A message is represented by \n",
    "$$\n",
    "x^{(i)}=(x_1^{(i)},x_2^{(i)},..., x_n^{(i)})\n",
    "$$ \n",
    "where $x_j^{(i)} \\le |V|$ is a word id of the $j$th word in the message. This representation has a much smaller dimension compared to the Naive Bayes representation.\n",
    "\n",
    "This model will have the same parameters as the above Naives Bayes model, except that the calculation of the conditional distribution parameter is slightly different:\n",
    "$$\n",
    "\\phi_{k|y=0}=p(x_j=k|y=0)\n",
    "$$\n",
    "The above definition assumes that the probability of word $k$ appears in the message is the same for all position in the message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn in high dimentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "Loss function:\n",
    "$$\n",
    "L(\\theta) = \\max(0, 1-y\\theta^Tx)\n",
    "$$\n",
    "\n",
    "If the predicted value and the actual value are of the same sign, the cost is 0. Otherwise we learn a $\\theta$ to push the margin to be a larger value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels\n",
    "\n",
    "#### General loss function $L(\\theta^Tx,y)$\n",
    "\n",
    "Consider linear regression and logistic regression  \n",
    "\n",
    "* Logistic loss (for logistic regress):\n",
    "$$\n",
    "L(z,y)=\\log(1+\\exp(-yz))\n",
    "$$\n",
    "\n",
    "* Squared error (for linear regress):\n",
    "$$\n",
    "L(z,y)=\\frac{1}{2}(z-y)^2\n",
    "$$\n",
    "\n",
    "And choose $\\theta$ by minimizing the empirical risk:\n",
    "$$\n",
    "J(\\theta)=\\frac{1}{m}\\sum_{i=1}^m L(\\theta x^{(i)}), y^{(i)})\n",
    "$$\n",
    "\n",
    "#### The representer theorem\n",
    "\n",
    "By adding an L2 norm to the above empirical risk $J(\\theta)$:\n",
    "$$\n",
    "J_\\lambda(\\theta)=\\frac{1}{m}\\sum_{i=1}^m L(\\theta x^{(i)}), y^{(i)})+\\frac{\\lambda}{2}||\\theta||_2^2\n",
    "$$\n",
    "\n",
    "##### Throrem\n",
    "There exists a minimizer of the above regularized risk that can be written as:\n",
    "$$\n",
    "\\theta=\\sum_{i=1}^m\\alpha_i x^{(i)}\n",
    "$$\n",
    "for some real valued $\\alpha_i$\n",
    "\n",
    "#### Kernel\n",
    "Based on the representer theorem, we can always write the vector $\\theta$ as a linear combination of the data $x^{(i)}$, which means we can learn and predict directly over $\\alpha$:\n",
    "$$\n",
    "\\theta^T x = \\sum_{i=1}^m \\alpha_i x^T x^{(i)}\n",
    "$$\n",
    "\n",
    "Here we can replace the inner product $x^T x^{(i)}$ with its corresponding $\\textbf{kernel}$ $K(x, x^{(i)})$:  \n",
    "\n",
    "$$\n",
    "K(x, x^{(i)})=\\phi(x)^T \\phi(x^{(i)})\n",
    "$$\n",
    "\n",
    "And use some typical $\\textbf{kernel functions}$ which are corresponding to infinite-dimensional vectors $\\phi$:  \n",
    "\n",
    "Gaussion or Radial Basis Function(RBF):  \n",
    "$$\n",
    "K(x,z)=\\exp\\left(-\\frac{1}{2\\tau^2}||x-z||_2^2 \\right)\n",
    "$$\n",
    "\n",
    "Min-kernel (applicable when $x\\in \\mathbb{R}$):\n",
    "$$\n",
    "K(x,z)=min{x,z}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixtures of Gaussian\n",
    "\n",
    "\n",
    "Training set $x^{(i)}$, the task is to do density estimation.  \n",
    "Suppose there is a laten variable $z^{(i)}$ which can be choosen from $k$ values, the density estimation can be modeled as:  \n",
    "\n",
    "$$\n",
    "z^{(i)} \\sim \\text{Multinomial}(\\phi)  \\\\\n",
    "x^{(i)}|z^{(i)}=j \\sim N(\\mu_j, \\Sigma_j)\n",
    "$$\n",
    "\n",
    "the random variables $z$ indicates there are $k$ Gaussians each $x^{(i)}$ is drawn from.\n",
    "\n",
    "Write out the log-likelihood:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\ell(\\phi,\\mu,\\Sigma)&=&\\sum_{i=1}^m \\log p(x^{(i)};\\phi, \\mu, \\Sigma) \\\\\n",
    "&=& \\sum_{i=1}^m \\log \\sum_{z_i=1}^k p(x^{(i)}|z^{(i)};\\mu,\\Sigma)p(z^{(i)},\\phi)\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
