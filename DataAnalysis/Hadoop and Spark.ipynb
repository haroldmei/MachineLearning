{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop import \\  \n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\  \n",
    "  --username retail_user \\  \n",
    "  --password itversity \\  \n",
    "  --table order_items \\  \n",
    "  --warehouse-dir /user/haroldmei/sqoop_import/retail_db \\  \n",
    "  --num-mappers 1 \\  \n",
    "  --delete-target-dir  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --table order_items \\\n",
    "  --warehouse-dir /user/haroldmei/sqoop_import/retail_db \\\n",
    "  --num-mappers 2 \\\n",
    "  --delete-target-dir \\\n",
    "  --as-textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --table order_items \\\n",
    "  --warehouse-dir /user/haroldmei/sqoop_import/retail_db \\\n",
    "  --num-mappers 2 \\\n",
    "  --as-textfile \\\n",
    "  --compress \\\n",
    "  --delete-target-dir \\\n",
    "  --compression-codec org.apache.hadoop.io.compress.GzipCodec\n",
    "    # org.apache.hadoop.io.compress.SnappyCodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -cat \\\n",
    "/user/haroldmei/sqoop_import/retail_db/order_items/part-m-00000.gz \\\n",
    "| gzip -d \\\n",
    "| hadoop fs -put - /user/haroldmei/sqoop_import/retail_db/order_items/demo.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all tables of a database to hdfs\n",
    "sqoop import-all-tables \\  \n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\  \n",
    "  --username retail_user \\  \n",
    "  --password itversity \\  \n",
    "  --warehouse-dir /user/haroldmei/sqoop_import/retail_db \\  \n",
    "  --autoreset-to-one-mapper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all tables of a database to hive\n",
    "sqoop import-all-tables \\  \n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\  \n",
    "  --username retail_user \\  \n",
    "  --password itversity \\  \n",
    "  --hive-import \\  \n",
    "  --hive-database haroldmei_sqoop_import \\  \n",
    "  --autoreset-to-one-mapper   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing data with hive query\n",
    "> create table daily_revenue as  \n",
    "                             > select order_date,sum(order_item_subtotal) daily_revenue  \n",
    "                             > from orders join order_items on  \n",
    "                             > order_id=order_item_order_id  \n",
    "                             > where order_date like '2013-07%'  \n",
    "                             > group by order_date limit 10;  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export hive table\n",
    "--export-dir: the table location in hdfs\n",
    "\n",
    "sqoop export --connect jdbc:mysql://ms.itversity.com:3306/retail_export \\  \n",
    "--username retail_user \\  \n",
    "--password itversity \\  \n",
    "--export-dir /apps/hive/warehouse/haroldmei_sqoop_import.db/daily_revenue \\  \n",
    "--input-fields-terminated-by \"\\001\" \\  \n",
    "--table daily_revenue  \n",
    "\n",
    "\n",
    "sqoop export --connect jdbc:mysql://ms.itversity.com:3306/retail_export \\\n",
    "--username retail_user --password itversity \\\n",
    "--export-dir /apps/hive/warehouse/haroldmei_sqoop_import.db/daily_revenue \\\n",
    "--table daily_revenue_demo \\\n",
    "--columns order_date,revenue \\\n",
    "--input-fields-terminated-by \"\\001\" \\\n",
    "--num-mappers 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "lines = sc.textFile(\"/public/randomtextwriter/part-m-00000\")\n",
    "lines.persist(StorageLevel.MEMORY_ONLY)\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"/public/randomtextwriter/part-m-00000\")\n",
    "def getWordTuples(i):\n",
    "  import itertools as it\n",
    "  # here plays the same role as spark flatMap\n",
    "  wordTuples = map(lambda s: (s, 1), it.chain.from_iterable(map(lambda s: s.split(\" \"), i)))\n",
    "  return wordTuples\n",
    "\n",
    "wordTuples = lines.mapPartitions(lambda i: getWordTuples(i))\n",
    "for i in wordTuples.reduceByKey(lambda x, y: x + y).take(10):\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read products data into RDD\n",
    "products = sc.textFile(\"/public/retail_db/products\")\n",
    "productsFiltered = products.filter(lambda p: p.split(\",\")[4] != \"\")\n",
    "\n",
    "# Convert the data to (k, v) using product category id as key and the entire product record as value\n",
    "# For each record in productsFiltered, column 1 is the category id, use it as key to this map.\n",
    "productsMap = productsFiltered.map(lambda p: (int(p.split(\",\")[1]), p))\n",
    "\n",
    "# Group by key which is just category id\n",
    "productsGBCategory = productsMap.groupByKey()\n",
    "\n",
    "# Get top N products by price in each category\n",
    "def getTopNProducts(products, topN):\n",
    "  return sorted(products, key=lambda k: float(k.split(\",\")[4]), reverse=True)[:topN]\n",
    "\n",
    "# Get products with top N prices in each category (N different prices, more than N products)\n",
    "def getTopNPricedProducts(products, topN):\n",
    "  import itertools as it\n",
    "  productPrices = sorted(set(map(lambda p: float(p.split(\",\")[4]), products)), reverse=True)[:topN]\n",
    "  productsSorted = sorted(products, key=lambda k: float(k.split(\",\")[4]), reverse=True)\n",
    "  return it.takewhile(lambda product: float(product.split(\",\")[4]) in productPrices, productsSorted)\n",
    "    \n",
    "\n",
    "# iterate productsGBCategory, <key=category id, value=records with the same id, type=pyspark.resultiterable.ResultIterable>\n",
    "topNProductsByCategory = productsGBCategory.flatMap(lambda p: getTopNProducts(list(p[1]), 3))\n",
    "for i in topNProductsByCategory.take(10):\n",
    "  print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
