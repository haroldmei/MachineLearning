{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. A/B testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Coefficient of determination $R^2$\n",
    "A data set has n values $y_1,...,y_n$, each associated with a predicted value $f_1,...,f_n$;   \n",
    "[Coefficient_of_determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
    "\n",
    "1. total sum of squares: $SS_{tot}=\\sum_i (y_i - \\overline{y})^2$\n",
    "2. regression sum of squares (explained sum of squares): $SS_{reg}=\\sum_i (f_i - \\overline{y})^2$\n",
    "3. residual sum of squares: $SS_{res}=\\sum_i (f_i - y_i)^2$\n",
    "4. coefficient of determination: $R^2=1-\\frac{SS_{res}}{SS_{tot}}$\n",
    "\n",
    "5. unexplained/explained variance:\n",
    "6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Goodness-of-fit Tests\n",
    "Check whether the data come from an assumed parametric model. Let $\\mathfrak{F}=\\{f(x;\\theta): \\theta \\in \\Theta \\}$ be a parametric model, suppose the data takes values on real line. Divide the line into k disjoint intervals $I_1,...,I_k$.   \n",
    "\n",
    "Let $$p_j(\\theta)=\\int_{I_j}f(x;\\theta)dx$$ \n",
    "where $\\theta=(\\theta_1,...,\\theta_s)$ are the parameters in the assumed model, $N_j$ be the observasions that fall into $I_j$.   \n",
    "\n",
    "The likelihood for $\\theta$ based on the counts $N_1,...,N_k$ is the multinomial likelihood  \n",
    "$$\n",
    "Q(\\theta)=\\prod_{j=1}^k p_j(\\theta)^{N_j}\n",
    "$$\n",
    "Maximizing $Q(\\theta)$ yields estimates $\\tilde \\theta = (\\tilde \\theta_1,...,\\tilde \\theta_s)$ of $\\theta$.  \n",
    "\n",
    "Now define the test:\n",
    "$$\n",
    "Q=\\sum_{j=1}^{k} \\frac{(N_j - np_j(\\tilde \\theta))^2}{np_j(\\tilde \\theta)}\n",
    "$$\n",
    "\n",
    "##### Theorem\n",
    "Let $H_0$ be the null hypothesis that the data are IID draws from the model $\\mathfrak{F}=\\{f(x;\\theta): \\theta \\in \\Theta \\}$. Under $H_0$, the statistic Q defined above converges in distribution to a $\\chi_{k-1-s}^2$ random variable. Thus the approx p-value for the test is $\\mathbb{P}(\\chi_{k-1-s}^2 > q)$ where q denotes the observed value of Q.\n",
    "\n",
    "Goodness of fit testing if reject $H_0$ we conclude we should not use the model, but if we do not reject $H_0$ we cannot conclude the model is correct. This is why it is better to use nonparametric methods whenever possible rather than parametric assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Curse of dimensionality\n",
    "1. High dimensions make everything far away from each other and hence make clustering hard. \n",
    "2. For example, to cover a fraction of the volumn of the data we need to capture a very wide range for each variable as the number of variables increases.\n",
    "3. The sampling density decreases exponentially as p increases and hence the data becomes much more sparse without significantly more data.\n",
    "4. We should apply PCA or other techniques to reduce dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Is more data always better?\n",
    "##### Statistically\n",
    "If the data is biased, just getting more data won’t help.  \n",
    "If the model suffers from high bias, getting more data won’t improve the test results beyond a point. Model itself needs improvement.  \n",
    "\n",
    "##### Practically\n",
    "There’s a tradeoff between having more data and the additional storage, computational power, memory it requires. Hence, always think about the cost of having more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Advantages of plotting data before analysis\n",
    "1. Find data set errors\n",
    "2. Find skewness, outliers etc. (arithmetic mean, standard deviation)\n",
    "3. If data is multimodal, anything based on its mean or median is going to be suspect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. How can you make sure not analyze something that ends up meaningless?\n",
    "1. Exploratory phase\n",
    "Come up with lots of possible hypothesis, getting rough ideas of what hypothesis we want to pursue further.\n",
    "\n",
    "2. Exploitatory phase\n",
    "Look deeply into a set of hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What is the role of trial and error in data analysis? What is the the role of making a hypothesis before diving in?\n",
    "* Data analysis is a repetition of setting up a new hypothesis and trying to refute the null hypothesis.\n",
    "* The scientic method is eminently inductive: we elaborate hypothesis, test it and refute it or not. As a result, we come up with new hypotheses which are in turn tested and so on. This is an iterative process, as science always is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. How to determine which feature are most important in the model (Feature selection)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
